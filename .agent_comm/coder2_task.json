{
  "agent_id": "coder2",
  "task_id": "task_6",
  "files": [
    {
      "name": "setup.py",
      "purpose": "Package installation setup",
      "priority": "low"
    }
  ],
  "project_info": {
    "project_name": "enhanced_stat.ML_2507.22632v1_A_Unified_Analysis_of_Generalization_and_Sample_Co",
    "project_type": "computer_vision",
    "description": "Enhanced AI project based on stat.ML_2507.22632v1_A-Unified-Analysis-of-Generalization-and-Sample-Co with content analysis. Detected project type: computer vision (confidence score: 7 matches).",
    "key_algorithms": [
      "Bounded",
      "Layer",
      "Proved",
      "Current",
      "Improve",
      "Adaptation",
      "Statistical",
      "L-Layer",
      "Baseline",
      "Domain-Adversarial"
    ],
    "main_libraries": [
      "torch",
      "numpy",
      "pandas"
    ]
  },
  "paper_content": "\n--- chunk_2.txt ---\nPDF: stat.ML_2507.22632v1_A-Unified-Analysis-of-Generalization-and-Sample-Co.pdf\nChunk: 2/2\n==================================================\n\n--- Page 47 ---\nE Derivation of Lipschitz constants for common non-\nlinear activation functions\nHere we derive Lipschitz constants for some widely used nonlinear activation func-\ntions. Let \u03b7:Rdl\u2192Rdlrepresent an activation function in layer lgiving the output\n\u03b6=\u03b7(\u03be)for the input \u03be\u2208Rdl.\nE.1 ReLU activation\nWe begin with the rectified linear unit (ReLU) function \u03b7R:Rdl\u2192Rdlgiven by\n\u03b6(k) = max {0,\u03be(k)} (52)\nwhere \u03b6=\u03b7R(\u03be), and the notation (\u00b7)(k)denotes the k-th entry of a vector. For two\nvectors \u03be1,\u03be2\u2208Rdl, we have\n\u2225\u03b7R(\u03be1)\u2212\u03b7R(\u03be2)\u22252=dlX\nk=1(max{0,\u03be1(k)} \u2212max{0,\u03be2(k)})2\n\u2264dlX\nk=1(\u03be1(k)\u2212\u03be2(k))2=\u2225\u03be1\u2212\u03be2\u22252(53)\nwhere max{\u00b7,\u00b7}denotes the maximum of two scalar values. We thus get\n\u2225\u03b7R(\u03be1)\u2212\u03b7R(\u03be2)\u2225\u2264 \u2225\u03be1\u2212\u03be2\u2225\nwhich gives the Lipschitz constant of the ReLU function as LR= 1.\nE.2 Softplus activation\nNext, we consider the softplus function \u03b7SP:Rdl\u2192Rdlgiven by\n\u03b6(k) = log\u0010\n1 +e\u03be(k)\u0011\n(54)\nwhere \u03b6=\u03b7SP(\u03be). The derivative of the components of the softplus function can be\nupper bounded as\n\f\f\f\fd\ndtlog(1 + et)\f\f\f\f=\f\f\f\fet\n1 +et\f\f\f\f<1 (55)\nfor all t\u2208R. Then for \u03b61=\u03b7SP(\u03be1)and\u03b62=\u03b7SP(\u03be2)with\u03be1,\u03be2\u2208Rdl, from the\nmean value theorem we get\n|\u03b61(k)\u2212\u03b62(k)|\u2264 |\u03be1(k)\u2212\u03be2(k)| (56)\nwhich implies\n\u2225\u03b7SP(\u03be1)\u2212\u03b7SP(\u03be2)\u2225\u2264 \u2225\u03be1\u2212\u03be2\u2225. (57)\nHence, we obtain the Lipschitz constant of the softplus function as LSP= 1.\n47\n\n--- Page 48 ---\nE.3 Softmax activation\nLastly, we consider the softmax function \u03b7SM:Rdl\u2192Rdlgiven by\n\u03b7SM(\u03be) = [\u03b71\nSM(\u03be)\u03b72\nSM(\u03be)\u00b7\u00b7\u00b7\u03b7dl\nSM(\u03be)]T\nwhere \u03be\u2208Rdland each k-th component \u03b7k\nSM(\u03be) :Rdl\u2192Rof the softmax activation\nis defined as\n\u03b7k\nSM(\u03be) =e\u03be(k)\nPdl\nn=1e\u03be(n). (58)\nSince the functions \u03b7k\nSM(\u03be)are differentiable for all k, for any two \u03be1,\u03be2\u2208Rdl, it\nfollows from the multivariable mean value theorem that there exists some \u03be\u2208Rdl\nlying in the line segment between \u03be1and\u03be2such that\n\u03b7k\nSM(\u03be1)\u2212\u03b7k\nSM(\u03be2) = (\u2207\u03b7k\nSM(\u03be))T(\u03be1\u2212\u03be2)\nwhere \u2207\u03b7k\nSM(\u03be)\u2208Rdldenotes the gradient of \u03b7k\nSMat\u03be. The following inequality is\nthen obtained\n|\u03b7k\nSM(\u03be1)\u2212\u03b7k\nSM(\u03be2)|\u2264sup\n\u03be\u2208Rdl\u2225\u2207\u03b7k\nSM(\u03be)\u2225\u2225\u03be1\u2212\u03be2\u2225. (59)\nIn the sequel, in order to find a Lipschitz constant for the softmax function, we derive\na bound on the norm \u2225\u2207\u03b7k\nSM(\u03be)\u2225of its gradient.\nFor the case k\u0338=n, the derivative of \u03b7k\nSM(\u03be)with respect to the n-th entry \u03be(n)of\n\u03be\u2208Rdlis obtained as\n\u2202\u03b7k\nSM(\u03be)\n\u2202\u03be(n)=\u2202\n\u2202\u03be(n) \ne\u03be(k)\nPdl\nr=1e\u03be(r)!\n=\u2212e\u03be(k)e\u03be(n)\n\u0010Pdl\nr=1e\u03be(r)\u00112.\nSince all e\u03be(1), . . . , e\u03be(dl)are positive, it is easy to show that (e\u03be(1)+. . . ,+e\u03be(dl))2\u2265\n4e\u03be(k)e\u03be(n). Using this in the above expression, we get the bound\n\f\f\f\f\u2202\u03b7k\nSM(\u03be)\n\u2202\u03be(n)\f\f\f\f\u22641\n4. (60)\nNext, for the case k=n, we have\n\u2202\u03b7k\nSM(\u03be)\n\u2202\u03be(k)=\u2202\n\u2202\u03be(k) \ne\u03be(k)\nPdl\nr=1e\u03be(r)!\n= \ne\u03be(k)\nPdl\nr=1e\u03be(r)! \n1\u2212e\u03be(k)\nPdl\nr=1e\u03be(r)!\n.\nLetting \u03b1=e\u03be(k)/Pdl\nr=1e\u03be(r)in the above expression and observing that the maxi-\nmum value of the function \u03b1(1\u2212\u03b1)in the interval \u03b1\u2208[0,1]is1/4, we get\n\f\f\f\f\u2202\u03b7k\nSM(\u03be)\n\u2202\u03be(k)\f\f\f\f\u22641\n4. (61)\n48\n\n--- Page 49 ---\nCombining the results (60) and (61), the gradient of \u03b7k\nSM(\u03be)can be bounded as\n\u2225\u2207\u03b7k\nSM(\u03be)\u2225\u2264\u221adl\n4\nfor any \u03be\u2208Rdl. Using this in (59) gives\n|\u03b7k\nSM(\u03be1)\u2212\u03b7k\nSM(\u03be2)|\u2264\u221adl\n4\u2225\u03be1\u2212\u03be2\u2225\nfor any \u03be1,\u03be2\u2208Rdl, which implies\n\u2225\u03b7SM(\u03be1)\u2212\u03b7SM(\u03be2)\u2225\u2264dl\n4\u2225\u03be1\u2212\u03be2\u2225.\nDefining\ndmax= max\nl=1,...,Ldl\nwe thus get the Lipschitz constant of the softmax function as LSM=dmax/4.\nF Proof of Lemma 6\nProof. We prove the statements only for FsandGsas the proofs for the target domain\nare similar. We first show that Fsis compact with respect to the metric ds\nX. Let\n\u03a6s={\u0398s= (\u0398s1, . . . ,\u0398sL) :|\u0398sl\nij|\u2264A\u0398,\u2200i, j, l}\ndenote the parameter space over which the source network parameters are defined.\nRegarding \u03a6sas the Cartesian product of the corresponding matrix spaces at layers\nl= 1, . . . , L , it follows from the bound |\u0398sl\nij|\u2264A\u0398on the network parameters that the\nfinite dimensional set \u03a6sis closed and bounded, hence compact.\nWe next define a mapping MFs:\u03a6s\u2192 Fssuch that\nMFs(\u0398s) =fs\n\u0398s= (fs1\n\u0398s, . . . , fs(L\u22121)\n\u0398s) (62)\nwhere the notation fs\n\u0398s(xs)stands for the function fs(xs)defined in (19) by explic-\nitly referring to its dependence on the network parameters \u0398s. In the following, we\nshow that the mapping MFsis continuous. Let us consider a sequence {\u0398s\nn} \u2282\u03a6s\nconverging to an element \u0398s\n\u2217\u2208\u03a6s. Since the relation (14) between the features of ad-\njacent layers is given by a linear mapping followed by a continuous activation function\n\u03b7l, the mapping \u03besl\n\u0398s(xs)is a continuous function of \u0398s, i.e.\nlim\nn\u2192\u221e\u03besl\n\u0398sn(xs) =\u03besl\n\u0398s\u2217(xs). (63)\nIn fact, due to the assumptions on the boundedness (15) of the source samples, the\nboundedness (16) of the network parameters, and the Lipschitz continuity (25) of the\n49\n\n--- Page 50 ---\nactivation functions \u03b7l, it is easy to show that the convergence in (63) is uniform on\nXs. Hence, for any given \u03f5 >0, one can find some n0such that for n\u2265n0, we have\n\u2225\u03besl\n\u0398sn(xs)\u2212\u03besl\n\u0398s\u2217(xs)\u2225< \u03f5\nfor all xs\u2208 Xs, forl= 1, . . . , L \u22121. Then we have\n\u2225fsl\n\u0398sn(xs)\u2212fsl\n\u0398s\u2217(xs)\u22252\nXl=\u2225\u03d5l(\u03besl\n\u0398sn(xs))\u2212\u03d5l(\u03besl\n\u0398s\u2217(xs))\u22252\nXl\n=kl(\u03besl\n\u0398sn(xs),\u03besl\n\u0398sn(xs))\u22122kl(\u03besl\n\u0398sn(xs),\u03besl\n\u0398s\u2217(xs)) +kl(\u03besl\n\u0398s\u2217(xs),\u03besl\n\u0398s\u2217(xs))\n\u22642LK\u2225\u03besl\n\u0398sn(xs)\u2212\u03besl\n\u0398s\u2217(xs)\u2225<2LK\u03f5\nfor all xs\u2208 Xsdue to the Lipschitz continuity of the kernels kl. This gives\n\u2225fs\n\u0398sn(xs)\u2212fs\n\u0398s\u2217(xs)\u22252\nX=L\u22121X\nl=1\u2225fsl\n\u0398sn(xs)\u2212fsl\n\u0398s\u2217(xs)\u22252\nXl<2(L\u22121)LK\u03f5.\nWe have thus obtained\n\u2225fs\n\u0398sn(xs)\u2212fs\n\u0398s\u2217(xs)\u2225X<p\n2(L\u22121)LK\u221a\u03f5\nfor all n\u2265n0and for all xs\u2208 Xs, which shows that fs\n\u0398sn(xs)converges to fs\n\u0398s\u2217(xs)\nuniformly on Xs. Then we have\nlim\nn\u2192\u221eds\nX(fs\n\u0398s\nn, fs\n\u0398s\n\u2217) = lim\nn\u2192\u221esup\nxs\u2208Xs\u2225fs\n\u0398s\nn(xs)\u2212fs\n\u0398s\n\u2217(xs)\u2225X\n= sup\nxs\u2208Xslim\nn\u2192\u221e\u2225fs\n\u0398sn(xs)\u2212fs\n\u0398s\u2217(xs)\u2225X= 0\nwhere the second equality follows from the uniform convergence of fs\n\u0398s\nn(xs). We have\nthus shown that the mapping MFs:\u03a6s\u2192 Fsdefined in (62) is continuous. Since the\nset\u03a6sis compact, we conclude that the function space Fsis a compact metric space.\nNext, in order to show the compactness of Gs, we proceed in a similar fashion. Let\nus define a mapping MGs:\u03a6s\u2192 GswithMGs(\u0398s) =gs\n\u0398s, where the notation\ngs\n\u0398s(xs) =\u03besL\n\u0398s(xs)refers to the network output function defined in (20) by clarifying\nits dependence on the network parameters. Similarly to (63), it is easy to observe that\n\u03besL\n\u0398s(xs)is a continuous function of \u0398sand for any sequence {\u0398s\nn}converging to an\nelement \u0398s\n\u2217\u2208\u03a6s\nlim\nn\u2192\u221egs\n\u0398sn(xs) = lim\nn\u2192\u221e\u03besL\n\u0398sn(xs) =\u03besL\n\u0398s\u2217(xs) =gs\n\u0398s\u2217(xs)\nuniformly. Hence,\nlim\nn\u2192\u221eds(gs\n\u0398sn, gs\n\u0398s\u2217) = lim\nn\u2192\u221esup\nxs\u2208Xs\u2225gs\n\u0398sn(xs)\u2212gs\n\u0398s\u2217(xs)\u2225\n= sup\nxs\u2208Xslim\nn\u2192\u221e\u2225gs\n\u0398sn(xs)\u2212gs\n\u0398s\u2217(xs)\u2225= 0.\nHence, the mapping MGs:\u03a6s\u2192 Gsis continuous. Then, from the compactness of\n\u03a6s, it follows that the function space Gsis compact as well.\n50\n\n--- Page 51 ---\nG Proof of Lemma 7\nProof. We obtain the bound only for the source domain, as the derivation for the target\ndomain is identical. Our proof is based on constructing an \u03f5-cover for the compact\nmetric space Fs. For two mappings fs\n1, fs\n2\u2208 Fsdefined respectively by the parameter\nvectors \u0398s\n1,\u0398s\n2we have\n(ds\nX(fs\n1, fs\n2))2= sup\nxs\u2208Xs\u2225fs\n1(xs)\u2212fs\n2(xs)\u22252\nX\n= sup\nxs\u2208XsL\u22121X\nl=1\u2225\u03d5l(\u03besl\n\u0398s\n1(xs))\u2212\u03d5l(\u03besl\n\u0398s\n2(xs))\u22252\nXl\n= sup\nxs\u2208XsL\u22121X\nl=1kl\u0010\n\u03besl\n\u0398s\n1(xs),\u03besl\n\u0398s\n1(xs)\u0011\n\u22122kl\u0010\n\u03besl\n\u0398s\n1(xs),\u03besl\n\u0398s\n2(xs)\u0011\n+kl\u0010\n\u03besl\n\u0398s\n2(xs),\u03besl\n\u0398s\n2(xs)\u0011\n\u2264sup\nxs\u2208XsL\u22121X\nl=1\f\f\fkl\u0010\n\u03besl\n\u0398s\n1(xs),\u03besl\n\u0398s\n1(xs)\u0011\n\u2212kl\u0010\n\u03besl\n\u0398s\n1(xs),\u03besl\n\u0398s\n2(xs)\u0011\f\f\f\n+\f\f\fkl\u0010\n\u03besl\n\u0398s\n2(xs),\u03besl\n\u0398s\n2(xs)\u0011\n\u2212kl\u0010\n\u03besl\n\u0398s\n1(xs),\u03besl\n\u0398s\n2(xs)\u0011\f\f\f\n\u2264sup\nxs\u2208XsL\u22121X\nl=12LK\u2225\u03besl\n\u0398s\n1(xs)\u2212\u03besl\n\u0398s\n2(xs)\u2225\n(64)\nwhere the last inequality is due to the Lipschitz continuity of the kernels kl. We next\nconstruct a cover for the set of parameter vectors \u0398s, which will define a cover for Fs\nusing the relation in (64). From (16) the network parameter vectors of layer lare in the\ncompact set\n\u0398l={\u0398l= [Wlbl]\u2208Rdl\u00d7(dl\u22121+1):|Wl\nij|\u2264A\u0398,|bl\ni|\u2264A\u0398,\u2200i, j, l}. (65)\nThen there exists a cover of \u0398lconsisting of open balls around a set Gl={\u0398l\nm}\u03bal\nm=1\nof regularly sampled grid points, with a distance of \u03b4between adjacent grid centers\nin each dimension. The maximal overall distance between two adjacent grid centers\nis then \u03b4p\ndl(dl\u22121+ 1) . Hence, the distance between any parameter vector \u0398l\u2208\u0398l\nand the nearest grid center \u0398l\nmis at most\n\u03b4p\ndl(dl\u22121+ 1)\n2\nwith the number of balls in the cover being\n\u03bal=\u00122A\u0398\n\u03b4+ 1\u0013dl(dl\u22121+1)\n.\n51\n\n--- Page 52 ---\nFrom the Cartesian product of the grid centers at layers l= 1, . . . , L \u22121, we then\nobtain a product grid\nG=G1\u00d7. . .\u00d7GL\u22121={\u0398k}\u03ba1... \u03baL\u22121\nk=1(66)\nwhich defines a cover for the overall parameter space\n\u03a6={\u0398= (\u03981, . . . ,\u0398L\u22121) :|\u0398l\nij|\u2264A\u0398,\u2200i, j, l}\nconsisting of\n\u03baG=L\u22121Y\nl=1\u03bal=L\u22121Y\nl=1\u00122A\u0398\n\u03b4+ 1\u0013dl(dl\u22121+1)\nballs. Then for any fs\u2208 Fswith parameters \u0398s, there exists some fs\nk\u2208 Fswith\nparameters \u0398k= (\u03981\nk,\u03982\nk, . . . ,\u0398L\u22121\nk)\u2208Gin the product grid such that\n\u2225\u0398sl\u2212\u0398l\nk\u2225< \u03b4p\ndl(dl\u22121+ 1). (67)\nFor any xs\u2208 Xs, the distance between the l-th layer features of these parameters can\nbe bounded as\n\u2225\u03besl\n\u0398s(xs)\u2212\u03bel\n\u0398k(xs)\u2225=\n\n\n\u03b7l\u0010\nWsl\u03bes(l\u22121)\n\u0398s(xs) +bsl\u0011\n\u2212\u03b7l\u0000\nWl\nk\u03bel\u22121\n\u0398k(xs) +bl\nk\u0001\n\n\n\u2264L\u03b7\n\n\nWsl\u03bes(l\u22121)\n\u0398s(xs) +bsl\u2212Wl\nk\u03bel\u22121\n\u0398k(xs)\u2212bl\nk\n\n\n=L\u03b7\n\n\nWsl\u03bes(l\u22121)\n\u0398s(xs)\u2212Wsl\u03bel\u22121\n\u0398k(xs) +Wsl\u03bel\u22121\n\u0398k(xs)\u2212Wl\nk\u03bel\u22121\n\u0398k(xs) +bsl\u2212bl\nk\n\n\n\u2264L\u03b7\u2225Wsl\u2225\u2225\u03bes(l\u22121)\n\u0398s(xs)\u2212\u03bel\u22121\n\u0398k(xs)\u2225+L\u03b7\u2225Wsl\u2212Wl\nk\u2225\u2225\u03bel\u22121\n\u0398k(xs)\u2225+L\u03b7\u2225bsl\u2212bl\nk\u2225\n(68)\nwhere Wl\nk,bl\nk, and \u03bel\u22121\n\u0398kdenote the l-th layer network parameters and features gen-\nerated by the parameter vector \u0398k; and\u2225\u00b7\u2225and\u2225\u00b7\u2225Frespectively denote the operator\nnorm and the Frobenius norm of a matrix. From (65) and (67), we have\n\u2225Wsl\u2225 \u2264 \u2225Wsl\u2225F\u2264A\u0398p\ndldl\u22121\n\u2225Wsl\u2212Wl\nk\u2225 \u2264 \u2225Wsl\u2212Wl\nk\u2225F< \u03b4p\ndldl\u22121\n\u2225bsl\u2212bl\nk\u2225< \u03b4p\ndl.\nThese bounds together with the inequality in (68) yield\n\u2225\u03besl\n\u0398s(xs)\u2212\u03bel\n\u0398k(xs)\u2225< L\u03b7A\u0398p\ndldl\u22121\u2225\u03bes(l\u22121)\n\u0398s(xs)\u2212\u03bel\u22121\n\u0398k(xs)\u2225\n+L\u03b7\u03b4p\ndldl\u22121\u2225\u03bel\u22121\n\u0398k(xs)\u2225+L\u03b7\u03b4p\ndl.(69)\n52\n\n--- Page 53 ---\nIn order to study (69), we first obtain an upper bound on the term \u2225\u03bel\n\u0398k(xs)\u2225. Notice\nthat for the condition (26), we simply have\n\u2225\u03bel\n\u0398k(xs)\u2225=\u2225\u03b7l\u0000\nWl\u03bel\u22121\n\u0398k(xs) +bl\u0001\n\u2225= dlX\ni=1\u0000\n\u03b7l\ni(Wl\u03bel\u22121\n\u0398k(xs) +bl)\u00012!1/2\n\u2264C\u03b7p\ndl.\n(70)\nNext, for the condition (27) we have\n\u2225\u03be0\n\u0398k(xs)\u2225=\u2225xs\u2225\u2264Ax\n\u2225\u03be1\n\u0398k(xs)\u2225=\u2225\u03b71\u0000\nW1\u03be0\n\u0398k(xs) +b1\u0001\n\u2225\u2264A\u03b7\u2225W1\u03be0\n\u0398k(xs) +b1\u2225\n\u2264A\u03b7(\u2225W1\u2225\u2225\u03be0\n\u0398k(xs)\u2225+\u2225b1\u2225)\u2264A\u03b7A\u0398p\nd1d0Ax+A\u03b7A\u0398p\nd1\nfor layers l= 0 andl= 1. For l\u22652, one can similarly establish a recursive relation\nbetween the parameter vectors of layers landl\u22121, which yields\n\u2225\u03bel\n\u0398k(xs)\u2225 \u2264A\u03b7\u0000\n\u2225Wl\u2225\u2225\u03bel\u22121\n\u0398k(xs)\u2225+\u2225bl\u2225\u0001\n\u2264A\u03b7A\u0398p\ndldl\u22121\u2225\u03bel\u22121\n\u0398k(xs)\u2225+A\u03b7A\u0398p\ndl\n\u2264(A\u03b7A\u0398)l(Axp\nd0+ 1)p\nd1l\u22121Y\nk=1p\ndk+1dk\n+l\u22121X\ni=2(A\u03b7A\u0398)l+1\u2212ip\ndil\u22121Y\nk=1p\ndk+1dk+A\u03b7A\u0398p\ndl.\nHence, combining this with (70), we get\n\u2225\u03bel\n\u0398k(xs)\u2225\u2264Rl (71)\nforl= 2, . . . , L \u22121, where Rlis the constant defined in Lemma 7. Using this in (69),\nwe obtain\n\u2225\u03besl\n\u0398s(xs)\u2212\u03bel\n\u0398k(xs)\u2225< L\u03b7A\u0398p\ndldl\u22121\u2225\u03bes(l\u22121)\n\u0398s(xs)\u2212\u03bel\u22121\n\u0398k(xs)\u2225\n+L\u03b7\u03b4p\ndldl\u22121Rl\u22121+L\u03b7\u03b4p\ndl.(72)\nFor layer l= 1, we have\n\u2225\u03bes1\n\u0398s(xs)\u2212\u03be1\n\u0398k(xs)\u2225< L\u03b7A\u0398p\nd1d0\u2225\u03bes0\n\u0398s(xs)\u2212\u03be0\n\u0398k(xs)\u2225\n+L\u03b7\u03b4p\nd1d0R0+L\u03b7\u03b4p\nd1\n=L\u03b7\u03b4p\nd1d0R0+L\u03b7\u03b4p\nd1\n53\n\n--- Page 54 ---\nsince \u03bes0\n\u0398s(xs) =\u03be0\n\u0398k(xs) =xs. This relation together with the recursive inequality\nin (72) yields\n\u2225\u03besl\n\u0398s(xs)\u2212\u03bel\n\u0398k(xs)\u2225< \u03b4\u0012\n(L\u03b7Rl\u22121p\ndldl\u22121+L\u03b7p\ndl)\n+l\u22121X\ni=1(L\u03b7Ri\u22121p\ndidi\u22121+L\u03b7p\ndi)lY\nk=i+1L\u03b7A\u0398p\ndkdk\u22121\u0013\n=Ql\u03b4\n(73)\nforl= 1, . . . , L \u22121. Hence, we have shown that for any fs\u2208 Fswith parameters\n\u0398s, there exists some fs\nk\u2208 Fswith parameters \u0398k\u2208Gin the product grid such that\n\u2225\u03besl\n\u0398s(xs)\u2212\u03bel\n\u0398k(xs)\u2225< Q l\u03b4\nfor any xs\u2208 Xs. We can now use this in (64) to bound the distance ds\nX(fs, fs\nk)as\n(ds\nX(fs, fs\nk))2\u2264sup\nxs\u2208XsL\u22121X\nl=12LK\u2225\u03besl\n\u0398s(xs)\u2212\u03bel\n\u0398k(xs)\u2225<2LK\u03b4L\u22121X\nl=1Ql= 2LK\u03b4Q.\n(74)\nTherefore, the set {fs\nk}\u03baG\nk=1\u2282 Fsprovides a cover for Fswith covering radius\u221a2LK\u03b4Q.\nIn order to obtain a covering radius of \u03f5=\u221a2LK\u03b4Q, we set\n\u03b4=\u03f52\n2LKQ\nwhich provides a grid consisting of\nL\u22121Y\nl=1\u03bal=L\u22121Y\nl=1\u00124A\u0398LKQ\n\u03f52+ 1\u0013dl(dl\u22121+1)\nballs that covers Fs. Hence, we obtain the upper bound\nN(Fs, \u03f5,ds\nX)\u2264L\u22121Y\nl=1\u00124A\u0398LKQ\n\u03f52+ 1\u0013dl(dl\u22121+1)\nfor the covering number stated in the lemma.\nH Proof of Lemma 8\nProof. We prove the statement of the lemma only for the source function space H\u25e6Fs,\nas the derivations for the target domain are identical. In order to bound the covering\n54\n\n--- Page 55 ---\nnumber for H \u25e6 Fs, we proceed as in the proof of Lemma 7 and extend the grid con-\nstruction in (66) to include layer Las well. This defines a grid\nGH\u25e6F=G1\u00d7. . .\u00d7GL={\u0398k}\u03ba1... \u03baL\nk=1(75)\nproviding a cover for the parameter space\n\u03a6H\u25e6F={\u0398= (\u03981, . . . ,\u0398L) :|\u0398l\nij|\u2264A\u0398,\u2200i, j, l}\nconsisting of\nLY\nl=1\u03bal=LY\nl=1\u00122A\u0398\n\u03b4+ 1\u0013dl(dl\u22121+1)\n(76)\nballs. Then for any gs\u2208 H \u25e6 Fswith network parameters \u0398s, there exists some\ngs\nk\u2208 H \u25e6 Fswith network parameters \u0398k= (\u03981\nk,\u03982\nk, . . . ,\u0398L\nk)\u2208GH\u25e6F in the grid\nsuch that\n\u2225\u0398sl\u2212\u0398l\nk\u2225< \u03b4p\ndl(dl\u22121+ 1)\nforl= 1, . . . , L . Proceeding in a similar fashion to the derivations in (68) and (69),\nwe obtain\n\u2225\u03besL\n\u0398s(xs)\u2212\u03beL\n\u0398k(xs)\u2225 \u2264L\u03b7\u2225WsL\u2225\u2225\u03bes(L\u22121)\n\u0398s(xs)\u2212\u03beL\u22121\n\u0398k(xs)\u2225\n+L\u03b7\u2225WsL\u2212WL\nk\u2225\u2225\u03beL\u22121\n\u0398k(xs)\u2225+L\u03b7\u2225bsL\u2212bL\nk\u2225\n< L\u03b7A\u0398p\ndLdL\u22121\u2225\u03bes(L\u22121)\n\u0398s(xs)\u2212\u03beL\u22121\n\u0398k(xs)\u2225\n+L\u03b7\u03b4p\ndLdL\u22121\u2225\u03beL\u22121\n\u0398k(xs)\u2225+L\u03b7\u03b4p\ndL(77)\nfor any xs\u2208 Xs. Combining this inequality with the bounds in (71) and (73) gives\n\u2225\u03besL\n\u0398s(xs)\u2212\u03beL\n\u0398k(xs)\u2225< L\u03b7A\u0398p\ndLdL\u22121QL\u22121\u03b4\n+L\u03b7\u03b4p\ndLdL\u22121RL\u22121+L\u03b7\u03b4p\ndL\n=QL\u03b4.\nRecalling the definition of the distance dsin (4), we then have\nds(gs, gs\nk) = sup\nxs\u2208Xs\u2225gs(xs)\u2212gs\nk(xs)\u2225= sup\nxs\u2208Xs\u2225\u03besL\n\u0398s(xs)\u2212\u03beL\n\u0398k(xs)\u2225< Q L\u03b4.\nHence, the grid GH\u25e6F in (75) provides a cover for H \u25e6 Fswith covering radius QL\u03b4.\nFor a covering radius of \u03f5, we set \u03f5=QL\u03b4, which results in a cover with\nLY\nl=1\u00122A\u0398QL\n\u03f5+ 1\u0013dl(dl\u22121+1)\n(78)\nballs due to (76). We thus get the covering number upper bound\nN(H \u25e6 Fs, \u03f5,ds)\u2264LY\nl=1\u00122A\u0398QL\n\u03f5+ 1\u0013dl(dl\u22121+1)\nstated in the lemma.\n55\n\n--- Page 56 ---\nI Proof of Corollary 1\nProof. In order to analyze the dependence of N(Fs, \u03f5,ds\nX)ondandL, we first study\nhow the term Rlin Lemma 7 grows with the dimension dand the number of layers L.\nFor condition (26), we have\nRl=C\u03b7p\ndl=O(d1/2).\nFor condition (27), representing the relevant constant terms as cfor simplicity, we have\nRl=O((cd)l).\nWe next study the term Qlin (28). For condition (26), we obtain\nQl=O(cl\u22121dl+1\n2)\nwhich results in\nQ=O(cL\u22122dL\u22121\n2). (79)\nMeanwhile, condition (27) yields\nQl=O((l\u22121)cl\u22121dl)\nresulting in\nQ=O((L\u22122)cL\u22122dL\u22121). (80)\nFor simplicity, we may combine the results in (79) and (80) through a slightly more\npessimistic but brief common upper bound as\nQ=O(L cL\u22122dL)\nwhich is valid for both of the conditions in (26) and (27). Then, from the expressions\nof the covering numbers N(Fs, \u03f5,ds\nX)andN(Ft, \u03f5,dt\nX)in Lemma 7, we conclude\nN(Fs, \u03f5,ds\nX) =O \u0012cQ\n\u03f52\u0013d2L!\n=O \u0012L\n\u03f5\u0013d2L\n(cd)d2L2!\nwhere we have taken the liberty to replace the \u03f52term in the denominator with \u03f5for\nsimplicity, as they will lead to equivalent bounds. Similarly,\nN(Ft, \u03f5,dt\nX) =O \u0012L\n\u03f5\u0013d2L\n(cd)d2L2!\n.\nWe next analyze the covering number N(H \u25e6 Fs, \u03f5,ds)for the hypothesis space\nH \u25e6 Fs. For condition (26), we have\nQL=O(cL\u22121dL+1\n2)\n56\n\n--- Page 57 ---\nwhich gives from Lemma 8\nN(H \u25e6 Fs, \u03f5,ds) =O \u0012cQL\n\u03f5\u0013d2L!\n=O \n(cd)d2L2\n\u03f5d2L!\n(81)\nif the d2L/2term added to the d2L2term in the exponent is ignored for simplicity.\nNext, for condition (27) we obtain\nQL=O((L\u22121)cL\u22121dL)\nresulting in\nN(H \u25e6 Fs, \u03f5,ds) =O \u0012cQL\n\u03f5\u0013d2L!\n=O \u0012L\n\u03f5\u0013d2L\n(cd)d2L2!\n. (82)\nCombining the bounds in (81) and (82), we arrive at the common upper bound\nN(H \u25e6 Fs, \u03f5,ds) =O \u0012L\n\u03f5\u0013d2L\n(cd)d2L2!\nwhich covers both conditions. Identical derivations for the target domain yield\nN(H \u25e6 Ft, \u03f5,dt) =O \u0012L\n\u03f5\u0013d2L\n(cd)d2L2!\n.\nJ Proof of Theorem 3\nProof. We first notice that, owing to Lemma 5, we can analyze MMD-based domain\nadaptation networks within the setting of Theorem 2. The compactness of the function\nspaces Fs,Ft,H \u25e6 Fs, andH \u25e6 Ftfollow from Assumptions 5-7 due to Lemma 6.\nAssumptions 2 and 4 are thereby satisfied; hence, the statement of Theorem 2 applies\nto the current setting in consideration.\nWe recall from Theorem 2 that the expected target loss in (29) is attained with\nprobability at least\n1\u22122N(H \u25e6 Ft,\u03f5\n8\u03b1L\u2113,dt)e\u2212Mt\u03f52\n8\u03b12A2\n\u2113\u22122N(H \u25e6 Fs,\u03f5\n8(1\u2212\u03b1)L\u2113,ds)e\u2212Ms\u03f52\n8(1\u2212\u03b1)2A2\n\u2113\n\u2212 N(Fs,\u03f5\n8,ds\nX) exp(\u2212as(Ns, \u03f5))\u2212 N(Ft,\u03f5\n8,dt\nX) exp(\u2212at(Nt, \u03f5)).\n(83)\nOur proof is then based on identifying the rate at which the number of samples should\ngrow with Landdso that each one of the terms subtracted from 1 in the expression\n57\n\n--- Page 58 ---\n(83) remains fixed. This will in return guarantee that the generalization gap of O(\u03f5)in\n(29) be attained with high probability.\nWe begin with the term N(Fs,\u03f5\n8,ds\nX) exp(\u2212as(Ns, \u03f5)). Recalling the definition\nofas(Ns, \u03f5)from Lemma 4, we have\nas(Ns, \u03f5) =\u03b8(Ns\u03f52)\nwhere we use the notation \u03b8(\u00b7)to refer to asymptotic tight bounds. Combining this\nwith Corollary 1, we obtain\nN(Fs,\u03f5\n8,ds\nX) exp(\u2212as(Ns, \u03f5)) =O \u0012L\n\u03f5\u0013d2L\n(cd)d2L2exp(\u2212Ns\u03f52)!\n=O\u0012\nexp\u0012\nd2Llog\u0012L\n\u03f5\u0013\n+d2L2log(cd)\u2212Ns\u03f52\u0013\u0013\n.\nWe conclude that the total number Nsof source samples required to ensure a lower\nbound on the probability expression (83) scales as\nNs=O \nd2Llog\u0000L\n\u03f5\u0001\n+d2L2log(d)\n\u03f52!\n,\nyielding the sample complexity stated in the theorem. An identical derivation based on\nbounding the term N(Ft,\u03f5\n8,dt\nX) exp(\u2212at(Nt, \u03f5))shows that Nthas the same sample\ncomplexity.\nNext, we examine the terms involving the number of labeled samples. Proceeding\nsimilarly, we get\nN(H \u25e6 Ft,\u03f5\n8\u03b1L\u2113,dt)e\u2212Mt\u03f52\n8\u03b12A2\n\u2113=O \u0012L\u03b1\n\u03f5\u0013d2L\n(cd)d2L2exp\u0012\n\u2212Mt\u03f52\n\u03b12\u0013!\n=O\u0012\nexp\u0012\nd2Llog\u0012L\u03b1\n\u03f5\u0013\n+d2L2log(cd)\u2212Mt\u03f52\n\u03b12\u0013\u0013\n.\nRecalling that 0\u2264\u03b1\u22641, we conclude that upper bounding the choice of the weight\nparameter \u03b1by the rate\n\u03b1=O\uf8eb\n\uf8ed \nMt\u03f52\nd2Llog\u0000L\n\u03f5\u0001\n+d2L2log(d)!1/2\uf8f6\n\uf8f8\nensures that the probability term N(H \u25e6 Ft,\u03f5\n8\u03b1L\u2113,dt)e\u2212Mt\u03f52\n8\u03b12A2\n\u2113remain bounded.\nFinally, for the number of labeled samples in the source domain, we have\nN(H \u25e6 Fs,\u03f5\n8(1\u2212\u03b1)L\u2113,ds)e\u2212Ms\u03f52\n8(1\u2212\u03b1)2A2\n\u2113\n=O \u0012L(1\u2212\u03b1)\n\u03f5\u0013d2L\n(cd)d2L2exp\u0012\n\u2212Ms\u03f52\n(1\u2212\u03b1)2\u0013!\n=O\u0012\nexp\u0012\nd2Llog\u0012L(1\u2212\u03b1)\n\u03f5\u0013\n+d2L2log(cd)\u2212Ms\u03f52\n(1\u2212\u03b1)2\u0013\u0013\n.\n58\n\n--- Page 59 ---\nRecalling again the bound 0\u22641\u2212\u03b1\u22641, we observe that the sample complexity\nMs=O \nd2Llog\u0000L\n\u03f5\u0001\n+d2L2log(d)\n\u03f52!\nensures a lower bound on the probability expression (83), which concludes the proof\nof the theorem.\nK Derivation of the bound and the Lipschitz constant\nfor the cross-entropy loss\nWe first discuss the magnitude bound A\u2113for the widely used cross-entropy loss func-\ntion. Let y1,y2\u2208 Y \u2282 Rmbe two nonnegative label vectors in the label set Y=\n[0,1]\u00d7 \u00b7\u00b7\u00b7 \u00d7 [0,1]\u2282Rm. In its na \u00a8\u0131ve form, the cross-entropy loss between y1andy2\nis given by\n\u2113(y1,y2) =\u2212mX\nk=1log(y1(k))y2(k) (84)\nwhere y(k)denotes the k-th entry of the vector y. While the original form (84) of the\ncross-entropy loss is not bounded, often the following modification is made in order to\navoid numerical issues in practical implementations\n\u2113(y1,y2) =\u2212mX\nk=1log(y1(k) +\u03b4)y2(k)\nwhere 0< \u03b4 < 1is a positive constant. We then have\n|\u2113(y1,y2)|\u2264mX\nk=1|\u2212log(y1(k) +\u03b4)y2(k)|\u2264mmax{|log(\u03b4)|,log(1 + \u03b4)}.\nAssuming that \u03b4is very small, we get the following bound on the loss magnitude\n|\u2113(y1,y2)|\u2264A\u2113\u225cm|log(\u03b4)|.\nWe next derive the Lipschitz constant L\u2113of the cross-entropy loss function. For\nanyy,y1,y2\u2208 Y we have\n|\u2113(y1,y)\u2212\u2113(y2,y)|=\f\f\f\f\f\u2212mX\nk=1log(y1(k) +\u03b4)y(k) +mX\nk=1log(y2(k) +\u03b4)y(k)\f\f\f\f\f\n\u2264mX\nk=1|log(y2(k) +\u03b4)\u2212log(y1(k) +\u03b4)|.\n(85)\n59\n\n--- Page 60 ---\nFor any t\u2265\u03b4, we have\f\f\f\fd\ndtlog(t)\f\f\f\f=\f\f\f\f1\nt\f\f\f\f\u22641\n\u03b4\nwhich gives\n\f\f\f\flog(y2(k) +\u03b4)\u2212log(y1(k) +\u03b4)\ny2(k)\u2212y1(k)\f\f\f\f\u22641\n\u03b4\ndue to the mean value theorem. Using this in (85), we get\n|\u2113(y1,y)\u2212\u2113(y2,y)|\u2264mX\nk=1\u03b4\u22121|y2(k)\u2212y1(k)|\u2264\u03b4\u22121\u221am\u2225y2\u2212y1\u2225\nwhich shows that the cross-entropy loss is Lipschitz continuous with respect to the first\nargument with constant\nL\u2113\u225c\u03b4\u22121\u221am.\nL Proof of Lemma 9\nProof. Due to the assumption of compactness of the function classes VsandVt, there\nexists an \u03f5-cover of each function space. Let us denote the cover numbers of Vsand\nVtas\n\u03bas=N(Vs, \u03f5,ds\nV), \u03bat=N(Vt, \u03f5,dt\nV)\nrespectively, and the corresponding sets of ball centers as {vs\nk}\u03bas\nk=1and{vt\nl}\u03bat\nl=1. Then,\nfor any vs\u2208 Vsand any vt\u2208 Vtthere exist some vs\nk\u2208 Vsandvt\nl\u2208 Vtsuch that\nds\nV(vs, vs\nk) = sup\nxs\u2208Xs|vs(xs)\u2212vs\nk(xs)|< \u03f5\ndt\nV(vt, vt\nl) = sup\nxt\u2208Xt|vt(xt)\u2212vt\nl(xt)|< \u03f5.(86)\nLet us denote\nD(vs\nk, vt\nl)\u225c\f\fE[vs\nk(xs)]\u2212E[vt\nl(xt)]\f\f\n\u02c6D(vs\nk, vt\nl)\u225c\f\f\f\f\f\f1\nNsNsX\ni=1vs\nk(xs\ni)\u22121\nNtNtX\nj=1vt\nl(xt\nj)\f\f\f\f\f\f.\nTake any fs\u2208 Fs,ft\u2208 Ftand\u2206\u2208 D. We have\n|D\u2206(fs, ft)\u2212\u02c6D\u2206(fs, ft)|\n=|D\u2206(fs, ft)\u2212D(vs\nk, vt\nl) +D(vs\nk, vt\nl)\u2212\u02c6D(vs\nk, vt\nl) +\u02c6D(vs\nk, vt\nl)\u2212\u02c6D\u2206(fs, ft)|\n\u2264 |D\u2206(fs, ft)\u2212D(vs\nk, vt\nl)|+|D(vs\nk, vt\nl)\u2212\u02c6D(vs\nk, vt\nl)|+|\u02c6D(vs\nk, vt\nl)\u2212\u02c6D\u2206(fs, ft)|.\n(87)\n60\n\n--- Page 61 ---\nWe proceed by bounding each one of the three terms at the right hand side of the\ninequality in (87). The first term can be upper bounded as\n|D\u2206(fs, ft)\u2212D(vs\nk, vt\nl)|=\f\f|E[vs(xs)]\u2212E[vt(xt)]|\u2212|E[vs\nk(xs)]\u2212E[vt\nl(xt)]|\f\f\n\u2264\f\fE[vs(xs)]\u2212E[vt(xt)]\u2212E[vs\nk(xs)] +E[vt\nl(xt)]\f\f\n\u2264 |E[vs(xs)]\u2212E[vs\nk(xs)]|+|E[vt(xt)]\u2212E[vt\nl(xt)]|<2\u03f5\n(88)\nwhere the last inequality follows from (86). For the third term in (87), one can similarly\nshow that\n|\u02c6D(vs\nk, vt\nl)\u2212\u02c6D\u2206(fs, ft)|<2\u03f5. (89)\nWe lastly study the second term in (87). We have\n|D(vs\nk, vt\nl)\u2212\u02c6D(vs\nk, vt\nl)|\n=\f\f\f\f\f\f\f\fE[vs\nk(xs)]\u2212E[vt\nl(xt)]\f\f\u2212\f\f\f\f\f\f1\nNsNsX\ni=1vs\nk(xs\ni)\u22121\nNtNtX\nj=1vt\nl(xt\nj)\f\f\f\f\f\f\f\f\f\f\f\f\n\u2264\f\f\f\f\f\fE[vs\nk(xs)]\u2212E[vt\nl(xt)]\u22121\nNsNsX\ni=1vs\nk(xs\ni) +1\nNtNtX\nj=1vt\nl(xt\nj)\f\f\f\f\f\f\n\u2264\f\f\f\f\f1\nNsNsX\ni=1vs\nk(xs\ni)\u2212E[vs\nk(xs)]\f\f\f\f\f+\f\f\f\f\f\f1\nNtNtX\nj=1vt\nl(xt\nj)\u2212E[vt\nl(xt)]\f\f\f\f\f\f.(90)\nAs the domain discriminator is bounded due to Assumption 9, from Hoeffding\u2019s in-\nequality we have\nP \f\f\f\f\f1\nNsNsX\ni=1vs\nk(xs\ni)\u2212E[vs\nk(xs)]\f\f\f\f\f\u2265\u03f5!\n\u22642 exp\u0012\n\u2212Ns\u03f52\n2C2\nD\u0013\nfor a fixed vs\nk\u2208 Vs, and a similar inequality can be obtained for a fixed vt\nl\u2208 Vt.\nApplying the union bound over all ball centers {vs\nk}\u03bas\nk=1and{vt\nl}\u03bat\nl=1, we get that with\nprobability at least\n1\u22122\u03basexp\u0012\n\u2212Ns\u03f52\n2C2\nD\u0013\n\u22122\u03batexp\u0012\n\u2212Nt\u03f52\n2C2\nD\u0013\nwe have\n\f\f\f\f\f1\nNsNsX\ni=1vs\nk(xs\ni)\u2212E[vs\nk(xs)]\f\f\f\f\f< \u03f5 and\f\f\f\f\f\f1\nNtNtX\nj=1vt\nl(xt\nj)\u2212E[vt\nl(xt)]\f\f\f\f\f\f< \u03f5\nfor all ball centers, which implies from (90)\n|D(vs\nk, vt\nl)\u2212\u02c6D(vs\nk, vt\nl)|<2\u03f5.\n61\n\n--- Page 62 ---\nCombining this result with the bounds in (87)-(89), we get\nP \nsup\nfs\u2208Fs,ft\u2208Ft,\u2206\u2208D|D\u2206(fs, ft)\u2212\u02c6D\u2206(fs, ft)|\u22646\u03f5!\n\u22651\u22122\u03basexp\u0012\n\u2212Ns\u03f52\n2C2\nD\u0013\n\u22122\u03batexp\u0012\n\u2212Nt\u03f52\n2C2\nD\u0013\n.\nReplacing \u03f5with\u03f5/6, we get the statement of the lemma.\nM Proof of Theorem 4\nProof. We begin by bounding the expected target loss as\nLt(ft, h)\u2264 Ls(fs, h) +RAD\u2206(fs, ft)\nusing Assumption 12. It follows that\nLt(ft, h) =\u03b1Lt(ft, h) + (1 \u2212\u03b1)Lt(ft, h)\n\u2264\u03b1Lt(ft, h) + (1 \u2212\u03b1)\u0000\nLs(fs, h) +RAD\u2206(fs, ft)\u0001\n=L\u03b1(fs, ft, h) + (1 \u2212\u03b1)RAD\u2206(fs, ft).(91)\nWe next aim to upper bound the expected loss L\u03b1(fs, ft, h)and the expected dis-\ntribution distance D\u2206(fs, ft)in terms of their empirical counterparts. It follows from\nAssumptions 5 and 10 that the source hypothesis space Gs=H \u25e6 Fs, the target hy-\npothesis space Gt=H \u25e6 Ft, the source domain discriminator space Vs=D \u25e6 Fsand\nthe target domain discriminator space Vt=D \u25e6 Ftare compact with respect to the\nmetrics ds,dt,ds\nV,dt\nVrespectively, which can be shown by following similar steps as\nin the proof of Lemma 6 in Appendix F.\nDue to the compactness of Gs,Gtand the assumptions on the classification loss\nfunction \u2113, we have\nP \nsup\nfs\u2208Fs,ft\u2208Ft,h\u2208H|L\u03b1(fs, ft, h)\u2212\u02c6L\u03b1(fs, ft, h)|\u2264\u03f5!\n\u22651\u22122N(H \u25e6 Ft,\u03f5\n8\u03b1L\u2113,dt)e\u2212Mt\u03f52\n8\u03b12A2\n\u2113\u22122N(H \u25e6 Fs,\u03f5\n8(1\u2212\u03b1)L\u2113,ds)e\u2212Ms\u03f52\n8(1\u2212\u03b1)2A2\n\u2113\n(92)\nfrom Lemma 2. Similarly, the compactness of Vs,Vttogether with Assumption 9\nimplies that\nP \nsup\nfs\u2208Fs,ft\u2208Ft,\u2206\u2208D|D\u2206(fs, ft)\u2212\u02c6D\u2206(fs, ft)|\u2264\u03f5!\n\u22651\u22122N(Vs,\u03f5\n6,ds\nV) exp\u0012\n\u2212Ns\u03f52\n72C2\nD\u0013\n\u22122N(Vt,\u03f5\n6,dt\nV) exp\u0012\n\u2212Nt\u03f52\n72C2\nD\u0013(93)\n62\n\n--- Page 63 ---\ndue to Lemma 9.\nCombining the results in (91), (92), and (93), we get that with probability at least\n1\u22122N(H \u25e6 Fs,\u03f5\n8(1\u2212\u03b1)L\u2113,ds)e\u2212Ms\u03f52\n8(1\u2212\u03b1)2A2\n\u2113\u22122N(H \u25e6 Ft,\u03f5\n8\u03b1L\u2113,dt)e\u2212Mt\u03f52\n8\u03b12A2\n\u2113\n\u22122N(Vs,\u03f5\n6,ds\nV) exp\u0012\n\u2212Ns\u03f52\n72C2\nD\u0013\n\u22122N(Vt,\u03f5\n6,dt\nV) exp\u0012\n\u2212Nt\u03f52\n72C2\nD\u0013\n(94)\nthe expected target loss is bounded as\nLt(ft, h)\u2264\u02c6L\u03b1(fs, ft, h) + (1 \u2212\u03b1)RA\u02c6D\u2206(fs, ft) + (1 \u2212\u03b1)RA\u03f5+\u03f5.\nIn the sequel, we examine each one of the terms in the probability expression in\n(94). As for the covering numbers of H \u25e6 FsandH \u25e6 Ft, Assumptions 5, 8, and 10\nensure that the result in Lemma 8 applies to this setting as well, which implies that the\nrate of growth of N(H\u25e6Fs, \u03f5,ds)andN(H\u25e6Ft, \u03f5,dt)withLanddis upper bounded\nby\nO \u0012L\n\u03f5\u0013d2L\n(cd)d2L2!\ndue to Corollary 1. Then, following the very same steps as in the proof of Theorem 3,\nwe get that upper bounding the weight parameter \u03b1by\n\u03b1=O\uf8eb\n\uf8ed \nMt\u03f52\nd2Llog\u0000L\n\u03f5\u0001\n+d2L2log(d)!1/2\uf8f6\n\uf8f8,\ntogether with scaling Msat rate\nMs=O \nd2Llog\u0000L\n\u03f5\u0001\n+d2L2log(d)\n\u03f52!\nensures an upper bound on the terms\nN(H \u25e6 Fs,\u03f5\n8(1\u2212\u03b1)L\u2113,ds)e\u2212Ms\u03f52\n8(1\u2212\u03b1)2A2\n\u2113\nand\nN(H \u25e6 Ft,\u03f5\n8\u03b1L\u2113,dt)e\u2212Mt\u03f52\n8\u03b12A2\n\u2113\nin the probability expression in (94).\nThen, in order to analyze the covering numbers of VsandVt, we proceed with the\nfollowing reasoning: Noting the paralel between the structures of the domain discrim-\ninator and the feature extractor network parameters considered in Assumptions 10, 8\n63\n\n--- Page 64 ---\nand 11, we observe that the function space Vs=D \u25e6 Fshas an identical construction\nto the function space Gs=H \u25e6 Fs, if the metric\nds(gs\n1, gs\n2) = sup\nxs\u2208Xs\u2225gs\n1(xs)\u2212gs\n2(xs)\u2225\nbased on the Euclidean distance in Rmis replaced by its counterpart\nds\nV(vs\n1, vs\n2) = sup\nxs\u2208Xs|vs\n1(xs)\u2212vs\n2(xs)|\nwhich uses the Euclidean distance in Rinstead. Hence, the latter is a special case\nof the former that can be obtained by setting m= 1. Consequently, the analysis of\nthe covering number N(H \u25e6 Fs, \u03f5,ds)in Corollary 1 immediately applies to N(D \u25e6\nFs, \u03f5,ds\nV)as well, only by replacing the number of layers Lwith the total number of\nlayers L+K\u22121in the cascade network formed by the combination of the feature\nextractor and the domain discriminator networks. We thus get\nN(Vs, \u03f5,ds\nV) =O \u0012L+K\n\u03f5\u0013d2(L+K)\n(cd)d2(L+K)2!\nwhich yields\nN(Vs,\u03f5\n6,ds\nV) exp\u0012\n\u2212Ns\u03f52\n72C2\nD\u0013\n=O \u0012L+K\n\u03f5\u0013d2(L+K)\n(cd)d2(L+K)2exp\u0012\n\u2212Ns\u03f52\n72C2\nD\u0013!\n=O\u0012\nexp\u0012\nd2(L+K) log\u0012L+K\n\u03f5\u0013\n+d2(L+K)2log(cd)\u2212Ns\u03f52\n72C2\nD\u0013\u0013\n.(95)\nWe thus conclude that the sample complexity\nNs=O \nd2(L+K) log\u0000L+K\n\u03f5\u0001\n+d2(L+K)2log(d)\n\u03f52!\nensures an upper bound on the term (95). The same arguments also hold for the target\ndomain, resulting in the sample complexity\nNt=O \nd2(L+K) log\u0000L+K\n\u03f5\u0001\n+d2(L+K)2log(d)\n\u03f52!\nfor the number of target samples, which concludes the proof of the theorem.\nReferences\n[1] W. M. Kouw and M. Loog, \u201cA review of domain adaptation without target labels,\u201d\nIEEE Trans. Pattern Anal. Mach. Intell. , vol. 43, no. 3, pp. 766\u2013785, 2021.\n64\n\n--- Page 65 ---\n[2] K. Azizzadenesheli, A. Liu, F. Yang, and A. Anandkumar, \u201cRegularized learning\nfor domain adaptation under label shifts,\u201d in Int. Conf. Learning Representations ,\n2019.\n[3] R. Tachet des Combes et al., \u201cDomain adaptation with conditional distribution\nmatching and generalized label shift,\u201d in Neural Inf. Proc. Systems , 2020.\n[4] P. Singhal, R. Walambe, S. Ramanna, and K. Kotecha, \u201cDomain adaptation:\nChallenges, methods, datasets, and applications,\u201d IEEE Access , vol. 11, pp. 6973\u2013\n7020, 2023.\n[5] J. Huang, A. J. Smola, A. Gretton, K. M. Borgwardt, and B. Sch \u00a8olkopf, \u201cCor-\nrecting sample selection bias by unlabeled data,\u201d in Proc. Advances in Neural\nInformation Processing Systems 19 , 2006, pp. 601\u2013608.\n[6] Q. Sun, R. Chattopadhyay, S. Panchanathan, and J. Ye, \u201cA two-stage weighting\nframework for multi-source domain adaptation,\u201d in Proc. Advances in Neural\nInformation Processing Systems 24 , 2011, pp. 505\u2013513.\n[7] H. Daum \u00b4e III, \u201cFrustratingly easy domain adaptation,\u201d in Annual Meeting-\nAssociation for Computational Linguistics , 2007.\n[8] H. Daum \u00b4e III, A. Kumar, and A. Saha, \u201cCo-regularization based semi-supervised\ndomain adaptation,\u201d in Proc. Advances in Neural Information Processing Systems\n23, 2010, pp. 478\u2013486.\n[9] L. Duan, D. Xu, and I. W. Tsang, \u201cLearning with augmented features for hetero-\ngeneous domain adaptation,\u201d in Proc. 29th International Conference on Machine\nLearning , 2012.\n[10] M. Baktashmotlagh, M. T. Harandi, B. C. Lovell, and M. Salzmann, \u201cUnsuper-\nvised domain adaptation by domain invariant projection,\u201d in IEEE International\nConference on Computer Vision , 2013, pp. 769\u2013776.\n[11] S. J. Pan, I. W. Tsang, J. T. Kwok, and Q. Yang, \u201cDomain adaptation via transfer\ncomponent analysis,\u201d IEEE Trans. Neural Networks , vol. 22, no. 2, pp. 199\u2013210,\n2011.\n[12] T. Yao, Y . Pan, C. Ngo, H. Li, and T. Mei, \u201cSemi-supervised domain adaptation\nwith subspace learning for visual recognition,\u201d in IEEE Conference on Computer\nVision and Pattern Recognition , 2015, pp. 2142\u20132150.\n[13] M. Wang and W. Deng, \u201cDeep visual domain adaptation: A survey,\u201d Neurocom-\nputing , vol. 312, pp. 135\u2013153, 2018.\n[14] M. Long, Y . Cao, J. Wang, and M. I. Jordan, \u201cLearning transferable features with\ndeep adaptation networks,\u201d in Proc 32nd International Conference on Machine\nLearning , vol. 37, pp. 97\u2013105.\n65\n\n--- Page 66 ---\n[15] E. Tzeng, J. Hoffman, N. Zhang, K. Saenko, and T. Darrell, \u201cDeep\ndomain confusion: Maximizing for domain invariance,\u201d arXiv preprint:\nhttp://arxiv.org/abs/1412.3474 , 2014.\n[16] M. Ghifary, W. B. Kleijn, and M. Zhang, \u201cDomain adaptive neural networks\nfor object recognition,\u201d in Int. Conf. Artificial Intelligence , 2014, vol. 8862, pp.\n898\u2013904.\n[17] Y . Zeng et al., \u201cMultirepresentation dynamic adaptive network for cross-domain\nrolling bearing fault diagnosis in complex scenarios,\u201d IEEE Transactions on In-\nstrumentation and Measurement , vol. 74, pp. 1\u201316, 2025.\n[18] P. Wang et al., \u201cInformation maximizing adaptation network with label distribu-\ntion priors for unsupervised domain adaptation,\u201d IEEE Transactions on Multime-\ndia, vol. 25, pp. 6026\u20136039, 2023.\n[19] Z. Xia et al., \u201cMeta domain adaptation approach for multi-domain ranking,\u201d IEEE\nAccess , vol. 13, pp. 92921\u201392931, 2025.\n[20] B. Yang et al., \u201cPoint-to-set metric-gated mixture of experts for multisource do-\nmain adaptation fault diagnosis,\u201d IEEE Transactions on Neural Networks and\nLearning Systems , pp. 1\u201315, 2025.\n[21] Y . Ganin et al., \u201cDomain-adversarial training of neural networks,\u201d J. Mach.\nLearn. Res. , vol. 17, pp. 59:1\u201359:35, 2016.\n[22] E. Tzeng, J. Hoffman, K. Saenko, and T. Darrell, \u201cAdversarial discriminative\ndomain adaptation,\u201d in IEEE Conference on Computer Vision and Pattern Recog-\nnition, , 2017, pp. 2962\u20132971.\n[23] H. Tang and K. Jia, \u201cDiscriminative adversarial domain adaptation,\u201d in AAAI\nConference on Artificial Intelligence , 2020, pp. 5940\u20135947.\n[24] M. H. Zonoozi and V . Seydi, \u201cA survey on adversarial domain adaptation,\u201d Neural\nProcess. Lett. , vol. 55, no. 3, pp. 2429\u20132469, 2023.\n[25] M. Ghifary et al., \u201cDeep reconstruction-classification networks for unsupervised\ndomain adaptation,\u201d in European Conf. Comp. Vision, , 2016, vol. 9908, pp. 597\u2013\n613.\n[26] K. Bousmalis et al., \u201cDomain separation networks,\u201d in Adv. Neural Information\nProcessing Systems , 2016, pp. 343\u2013351.\n[27] M. H. P. Zonoozi, V . Seydi, and M. Deypir, \u201cAn unsupervised adversarial domain\nadaptation based on variational auto-encoder,\u201d Mach. Learn. , vol. 114, no. 5, pp.\n128, 2025.\n[28] B. Sun and K. Saenko, \u201cDeep CORAL: correlation alignment for deep domain\nadaptation,\u201d in European Conf. Comp. Vision , 2016, vol. 9915, pp. 443\u2013450.\n66\n\n--- Page 67 ---\n[29] N. Courty et al., \u201cOptimal transport for domain adaptation,\u201d IEEE Transactions\non Pattern Analysis and Machine Intelligence , vol. 39, no. 9, pp. 1853\u20131865,\n2017.\n[30] B. B. Damodaran et al., \u201cDeepjdot: Deep joint distribution optimal transport for\nunsupervised domain adaptation,\u201d in European Conf. Comp. Vision , 2018, vol.\n11208, pp. 467\u2013483.\n[31] M. El Hamri, Y . Bennani, and I. Falih, \u201cTheoretical guarantees for domain adap-\ntation with hierarchical optimal transport,\u201d Mach. Learn. , vol. 114, no. 5, pp. 119,\n2025.\n[32] I. Redko, E. Morvant, A. Habrard, M. Sebban, and Y . Bennani, \u201cA survey on do-\nmain adaptation theory,\u201d arXiv preprint: http://arxiv.org/abs/2004.11829 , 2020.\n[33] S. Ben-David, J. Blitzer, K. Crammer, and F. Pereira, \u201cAnalysis of representations\nfor domain adaptation,\u201d in Proc. Advances in Neural Information Processing\nSystems 19 , 2006, pp. 137\u2013144.\n[34] Y . Mansour, M. Mohri, and A. Rostamizadeh, \u201cDomain adaptation: Learning\nbounds and algorithms,\u201d in The 22nd Conference on Learning Theory , 2009.\n[35] Y . Zhang, T. Liu, M. Long, and M. I. Jordan, \u201cBridging theory and algorithm\nfor domain adaptation,\u201d in Proceedings of the 36th International Conference on\nMachine Learning , 2019, vol. 97, pp. 7404\u20137413.\n[36] S. Dhouib, I. Redko, and C. Lartizien, \u201cMargin-aware adversarial domain adap-\ntation with optimal transport,\u201d in Proc. Int. Conf. Machine Learning, , 2020, vol.\n119, pp. 2514\u20132524.\n[37] Z. Wang and Y . Mao, \u201cOn f-divergence principled domain adaptation: An im-\nproved framework,\u201d in Advances in Neural Information Processing Systems ,\n2024.\n[38] J. T. Zhou, I. W. Tsang, S. J. Pan, and M. Tan, \u201cMulti-class heterogeneous domain\nadaptation,\u201d Journal of Machine Learning Research , vol. 20, no. 57, pp. 1\u201331,\n2019.\n[39] Z. Fang, J. Lu, F. Liu, and G. Zhang, \u201cSemi-supervised heterogeneous domain\nadaptation: Theory and algorithms,\u201d IEEE Trans. Pattern Anal. Mach. Intell. , vol.\n45, no. 1, pp. 1087\u20131105, 2023.\n[40] X. Wang and J. Schneider, \u201cGeneralization bounds for transfer learning under\nmodel shift,\u201d in Proc. Conf. Uncertainty in Artificial Intelligence , 2015, pp. 922\u2013\n931.\n[41] T. Galanti, L. Wolf, and T. Hazan, \u201cA theoretical framework for deep transfer\nlearning,\u201d Information and Inference: A Journal of the IMA , vol. 5, no. 2, pp.\n159\u2013209, 04 2016.\n67\n\n--- Page 68 ---\n[42] D. McNamara and M. Balcan, \u201cRisk bounds for transferring representations with\nand without fine-tuning,\u201d in Proc. Int. Conf. Machine Learning, , 2017, vol. 70,\npp. 2373\u20132381.\n[43] Y . Jiao, H. Lin, Y . Luo, and J. Z. Yang, \u201cDeep transfer learning: Model framework\nand error analysis,\u201d arXiv preprint: http://arxiv.org/abs/2410.09383 , 2024.\n[44] P. L. Anthony, M. Bartlett, Neural Network Learning - Theoretical Foundations ,\nCambridge University Press, Cambridge, UK, 2002.\n[45] B. Neyshabur, R. Tomioka, and N. Srebro, \u201cNorm-based capacity control in\nneural networks,\u201d in Prof. 28th Conference on Learning Theory , 2015, vol. 40,\npp. 1376\u20131401.\n[46] C. Wei and T. Ma, \u201cData-dependent sample complexity of deep neural networks\nvia Lipschitz augmentation,\u201d in Advances in Neural Information Processing Sys-\ntems 32 , 2019, pp. 9722\u20139733.\n[47] G. Vardi, O. Shamir, and N. Srebro, \u201cThe sample complexity of one-hidden-layer\nneural networks,\u201d in Advances in Neural Information Processing Systems 35 ,\n2022.\n[48] A. Daniely and E. Granot, \u201cOn the sample complexity of two-layer networks:\nLipschitz vs. element-wise Lipschitz activation,\u201d in International Conference on\nAlgorithmic Learning Theory , 2024, vol. 237, pp. 505\u2013517.\n[49] E. Vural, \u201cGeneralization bounds for domain adaptation via domain transforma-\ntions,\u201d in IEEE Int. Workshop Machine Learning for Signal Processing , 2018, pp.\n1\u20136.\n[50] F. Cucker and S. Smale, \u201cOn the Mathematical Foundations of Learning,\u201d Bul-\nletin of the American Mathematical Society , vol. 39, pp. 1\u201349, 2002.\n[51] A. Gretton, K. M. Borgwardt, M. J. Rasch, B. Sch \u00a8olkopf, and A. J. Smola, \u201cA\nkernel two-sample test,\u201d J. Mach. Learn. Res. , vol. 13, pp. 723\u2013773, 2012.\n[52] N. Dunford and J.T. Schwartz, Linear Operators, Part 1: General Theory , Wiley\nClassics Library. Interscience Publishers Inc., New York, 1988.\n[53] M. Long, Z. Cao, J. Wang, and M. I. Jordan, \u201cConditional adversarial domain\nadaptation,\u201d in Advances in Neural Information Processing Systems , 2018, pp.\n1647\u20131657.\n[54] E. Tzeng, J. Hoffman, T. Darrell, and K. Saenko, \u201cSimultaneous deep transfer\nacross domains and tasks,\u201d in IEEE International Conference on Computer Vi-\nsion, 2015, pp. 4068\u20134076.\n[55] S. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, and J. Wortman, \u201cA\ntheory of learning from different domains,\u201d Machine Learning , vol. 79, no. 1-2,\npp. 151\u2013175, 2010.\n68\n\n--- Page 69 ---\n[56] Y . Deng et al., \u201cOn the hardness of robustness transfer: A perspective from\nRademacher complexity over symmetric difference hypothesis space,\u201d arXiv\npreprint: http://arxiv.org/abs/2302.12351 , 2023.\n[57] W. Zellinger, B. A. Moser, and S. Saminger-Platz, \u201cOn generalization in moment-\nbased domain adaptation,\u201d Ann. Math. Artif. Intell. , vol. 89, no. 3-4, pp. 333\u2013369,\n2021.\n[58] Z. Wang and Y . Mao, \u201cInformation-theoretic analysis of unsupervised domain\nadaptation,\u201d in Int. Conf. Learning Representations , 2023.\n[59] X. Wu, J. H. Manton, U. Aickelin, and J. Zhu, \u201cOn the generalization for transfer\nlearning: An information-theoretic analysis,\u201d IEEE Trans. Inf. Theory , vol. 70,\nno. 10, pp. 7089\u20137124, 2024.\n[60] A. Sicilia, K. Atwell, M. Alikhani, and S. J. Hwang, \u201cPAC-Bayesian domain\nadaptation bounds for multiclass learners,\u201d in Proc. Conf. Uncertainty in Artificial\nIntelligence , 2022, vol. 180, pp. 1824\u20131834.\n[61] B. Wang et al., \u201cGap minimization for knowledge sharing and transfer,\u201d J. Mach.\nLearn. Res. , vol. 24, pp. 33:1\u201333:57, 2023.\n[62] M. Mohri and A. M. Medina, \u201cNew analysis and algorithm for learning with\ndrifting distributions,\u201d in Int. Conf. Algorithmic Learning Theory , 2012, vol.\n7568, pp. 124\u2013138.\n[63] N. Tripuraneni, M. I. Jordan, and C. Jin, \u201cOn the theory of transfer learning:\nThe importance of task diversity,\u201d in Advances in Neural Information Processing\nSystems , 2020.\n[64] P. L. Bartlett, A. Montanari, and A. Rakhlin, \u201cDeep learning: a statistical view-\npoint,\u201d Acta Numerica , vol. 30, pp. 87\u2013201, 2021.\n[65] B. Neyshabur, S. Bhojanapalli, and N. Srebro, \u201cA PAC-bayesian approach to\nspectrally-normalized margin bounds for neural networks,\u201d in Int. Conf. Learning\nRepresentations , 2018.\n[66] N. Golowich, A. Rakhlin, and O. Shamir, \u201cSize-independent sample complexity\nof neural networks,\u201d in Conference On Learning Theory , 2018, vol. 75, pp. 297\u2013\n299.\n[67] P. L. Bartlett, D. J. Foster, and M. Telgarsky, \u201cSpectrally-normalized margin\nbounds for neural networks,\u201d in Advances in Neural Information Processing Sys-\ntems 30 , 2017, pp. 6240\u20136249.\n[68] N. Harvey, C. Liaw, and A. Mehrabian, \u201cNearly-tight VC-dimension bounds for\npiecewise linear neural networks,\u201d in Proc. Conf. Learning Theory , 2017, vol. 65,\npp. 1064\u20131068.\n69\n\n--- Page 70 ---\n[69] Massachusetts Institute of Technology, \u201cMIT-CBCL face recognition database,\u201d\nAvailable: http://cbcl.mit.edu/software-datasets/heisele/facerecognition-\ndatabase.html.\n[70] B. Fernando, A. Habrard, M. Sebban, and T. Tuytelaars, \u201cUnsupervised visual\ndomain adaptation using subspace alignment,\u201d in IEEE International Conference\non Computer Vision , 2013, pp. 2960\u20132967.\n[71] Y . LeCun, L. Bottou, Y . Bengio, and P. Haffner, \u201cGradient-based learning applied\nto document recognition,\u201d Proceedings of the IEEE , vol. 86, no. 11, pp. 2278\u2013\n2324, 1998.\n[72] Y . Ganin and V . Lempitsky, \u201cUnsupervised domain adaptation by backpropaga-\ntion,\u201d in Proceedings of the 32nd International Conference on Machine Learning\n(ICML) , 2015, pp. 1180\u20131189.\n[73] H. Karaca et al., \u201cAn experimental study of the sample complexity of domain\nadaptation,\u201d in IEEE Signal Processing and Communications Applications Con-\nference , 2023, pp. 1\u20134.\n[74] C. Cai, \u201cDeep adaptation networks (DAN) in PyTorch,\u201d 2020, [Online]. Avail-\nable: https://github.com/CuthbertCai/pytorch_DAN . Accessed:\n2024-11-13.\n[75] GitHub repository, \u201cDann py3,\u201d 2023, [Online]. Available: https://\ngithub.com/fungtion/DANN_py3.git .\n[76] V . V . Yurinskii, \u201cExponential inequalities for sums of random vectors,\u201d Journal\nof Multivariate Analysis , vol. 6, no. 4, pp. 473\u2013499, 1976.\n[77] M. Subedi and J. Cortez, \u201cReproducing Kernel Hilbert Spaces - Part III,\u201d https:\n//www.math.uh.edu/ \u02dcdlabate/LectureNote_06.pdf , Accessed:\n2022-03-22.\n[78] V . I. Bogachev, Measure Theory , Springer, Berlin Heidelberg, 2007.\n[79] G. Bachman and L. Narici, Functional Analysis , Academic Press, New York and\nLondon, 1966.\n70\n",
  "project_dir": "artifacts/projects/enhanced_stat.ML_2507.22632v1_A_Unified_Analysis_of_Generalization_and_Sample_Co",
  "communication_dir": "artifacts/projects/enhanced_stat.ML_2507.22632v1_A_Unified_Analysis_of_Generalization_and_Sample_Co/.agent_comm",
  "assigned_at": "2025-07-31T21:27:26.683612",
  "status": "assigned"
}